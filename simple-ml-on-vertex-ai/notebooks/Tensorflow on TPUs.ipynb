{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LyhsCL9phAuU"
      },
      "source": [
        "# Tensorflow on TPUs with Vertex AI"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_NCvL3QhhGBb"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mugglmenzel/ml-code-examples/blob/main/simple-ml-on-vertex-ai/notebooks/Tensorflow%20on%20TPUs.ipynb)\n",
        "[![Open In Workbench](https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32)](https://console.cloud.google.com/ai-platform/notebooks/deploy-notebook?name=Tensorflow%2520on%2520GPUs&download_url=https%3A%2F%2Fraw.githubusercontent.com%2Fmugglmenzel%2Fml-code-examples%2Fmain%2Fsimple-ml-on-vertex-ai%2Fnotebooks%2FTensorflow%2520on%2520TPUs.ipynb)\n",
        "\n",
        "Contributor: michaelmenzel@google.com\n",
        "\n",
        "Disclaimer: This is a code example and not intended to be used in production. The author does not take any liability for the use of this code example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dRBaxb28yXE"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PsQq_qRCg4wC"
      },
      "source": [
        "## Train a TF model locally with TPU acceleration"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iniFgc8F9KLU"
      },
      "source": [
        "We start by running a simple training program with the MNIST dataset in Tensorflow:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8Sl0oLm_t9K"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "strategy = tf.distribute.TPUStrategy(resolver)\n",
        "print('Devices used by strategy in training loop:', strategy.extended.worker_devices)\n",
        "\n",
        "tf.get_logger().setLevel(logging.getLevelName('INFO'))\n",
        "logging.basicConfig(level=logging.getLevelName('INFO'))\n",
        "tf.debugging.set_log_device_placement(True)\n",
        "\n",
        "(train_data, val_data), mnist_info = tfds.load(\"mnist\",\n",
        "                                               split=['train', 'test'], as_supervised=True,\n",
        "                                               try_gcs=True, with_info=True)\n",
        "\n",
        "@tf.function\n",
        "def norm_data(image, label):\n",
        "    return tf.cast(image, tf.float32) / 255., label\n",
        "\n",
        "train_ds = (train_data\n",
        "            .map(norm_data, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "            .batch(128, drop_remainder=True)\n",
        "            .cache()\n",
        "            .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "val_ds = (val_data\n",
        "          .map(norm_data, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "          .batch(128, drop_remainder=True)\n",
        "          .cache()\n",
        "          .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "with strategy.scope():\n",
        "  model = keras.Sequential([\n",
        "          keras.layers.Reshape(target_shape=(28, 28, 1), input_shape=(28, 28)),\n",
        "          keras.layers.Conv2D(filters=64, kernel_size=(5, 5), padding='same', activation='elu'),\n",
        "          keras.layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2)),\n",
        "          keras.layers.Conv2D(filters=128, kernel_size=(5, 5), padding='same', activation='elu'),\n",
        "          keras.layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2)),\n",
        "          keras.layers.Conv2D(filters=256, kernel_size=(5, 5), padding='same', activation='elu'),\n",
        "          keras.layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2)),\n",
        "          keras.layers.Flatten(),\n",
        "          keras.layers.Dense(256, activation='elu'),\n",
        "          keras.layers.Dense(10, activation='softmax')\n",
        "      ])\n",
        "\n",
        "  model.compile(optimizer='adam', jit_compile=True,\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['sparse_categorical_accuracy'])\n",
        "\n",
        "model.fit(train_ds, validation_data=val_ds, epochs=1)\n",
        "model.evaluate(val_ds)\n",
        "model.save('my_model',\n",
        "           options=tf.saved_model.SaveOptions(\n",
        "               experimental_io_device='/job:localhost'))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "31kLYSrVgzyG"
      },
      "source": [
        "## Launch a TPU-accelerated Training on Vertex AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYTvDKCH9TXY"
      },
      "source": [
        "In this part we launch a training program on Vertex AI and register the resulting model in the Vertex AI Model Registry."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "x32sHjjPjZe5"
      },
      "outputs": [],
      "source": [
        "#@title Install Vertex AI Python SDK\n",
        "try:\n",
        "    from google.cloud import aiplatform\n",
        "except:\n",
        "    !pip install --user -q google-cloud-aiplatform\n",
        "    exit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "v8mkBVHXCJa5"
      },
      "outputs": [],
      "source": [
        "#@title Parameters\n",
        "from datetime import datetime\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "try:\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "except:\n",
        "    print('Not on Colab.')\n",
        "\n",
        "PROJECT_ID = 'sandbox-michael-menzel' #@param\n",
        "STAGING_BUCKET='gs://sandbox-michael-menzel-training-europe-west4/trainings/mnist-distributed-vertex' #@param\n",
        "\n",
        "\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "EXPERIMENT = f'{PROJECT_ID}-mnist-pysdk'\n",
        "JOB_NAME = f'{EXPERIMENT}-{TIMESTAMP}'\n",
        "\n",
        "\n",
        "aiplatform.init(location='europe-west4', project=PROJECT_ID, experiment=EXPERIMENT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdOnB9469dyT"
      },
      "source": [
        "Following we write the training script file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "NXHz7ksWhVcT"
      },
      "outputs": [],
      "source": [
        "%%writefile train.py\n",
        "#@title Write the training script\n",
        "\"\"\"\n",
        "Copyright 2023 Google LLC\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "    https://www.apache.org/licenses/LICENSE-2.0\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "logging.info(f\"Using Tensorflow version {tf.__version__}\")\n",
        "\n",
        "import hypertune\n",
        "\n",
        "hpt = hypertune.HyperTune()\n",
        "recorder = {'previous': 0, 'steps': []}\n",
        "\n",
        "def record(step, writer):\n",
        "    previous = recorder['steps'][recorder['previous']]['time'] if recorder['previous'] < len(recorder['steps']) else time.time()\n",
        "    current = time.time()\n",
        "    logging.info(f\"[{step}]: +{current - previous} sec ({current} UNIX)\")\n",
        "    with writer.as_default():\n",
        "        tf.summary.scalar(step, current, step=0)\n",
        "    hpt.report_hyperparameter_tuning_metric(\n",
        "                hyperparameter_metric_tag=step,\n",
        "                metric_value=current)\n",
        "    recorder['previous'] = len(recorder['steps']) - 1\n",
        "    recorder['steps'].append({'name': step, 'time': current})\n",
        "\n",
        "\n",
        "def summarize_recorder():\n",
        "    logging.info(\"Summary of processing steps (in seconds):\")\n",
        "    previous = 0\n",
        "    for step in recorder['steps']:\n",
        "        logging.info(f\"  Step: {step['name']}, Time: {step['time']}, Duration: {step['time'] - previous}\")\n",
        "        previous = step['time']\n",
        "\n",
        "\n",
        "class LossReporterCallback(tf.keras.callbacks.Callback):\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if logs:\n",
        "            print(f\"loss: {logs['loss']} in epoch: {epoch}\")\n",
        "            tf.summary.scalar('loss', logs['loss'], step=epoch)\n",
        "            hpt.report_hyperparameter_tuning_metric(\n",
        "                hyperparameter_metric_tag='loss',\n",
        "                metric_value=logs['loss'],\n",
        "                global_step=epoch)\n",
        "\n",
        "\n",
        "def _is_chief(strategy):\n",
        "    task_type = strategy.cluster_resolver.task_type\n",
        "    return task_type == 'chief' or task_type is None\n",
        "\n",
        "\n",
        "def _model_save_path(strategy):\n",
        "    if strategy.cluster_resolver:\n",
        "        task_type = strategy.cluster_resolver.task_type\n",
        "        task_id = strategy.cluster_resolver.task_id\n",
        "        subfolder = () if _is_chief(strategy) else (str(task_type), str(task_id))\n",
        "    else:\n",
        "        subfolder = ()\n",
        "    return os.path.join(os.environ['AIP_MODEL_DIR'], *subfolder)\n",
        "\n",
        "\n",
        "def _compile_model(strategy):\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.Reshape(target_shape=(28, 28, 1), input_shape=(28, 28)),\n",
        "        keras.layers.Conv2D(filters=64, kernel_size=(5, 5), padding='same', activation='elu'),\n",
        "        keras.layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2)),\n",
        "        keras.layers.Conv2D(filters=128, kernel_size=(5, 5), padding='same', activation='elu'),\n",
        "        keras.layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2)),\n",
        "        keras.layers.Conv2D(filters=256, kernel_size=(5, 5), padding='same', activation='elu'),\n",
        "        keras.layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2)),\n",
        "        keras.layers.Flatten(),\n",
        "        keras.layers.Dense(256, activation='elu'),\n",
        "        keras.layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    optimizer_config = {\n",
        "        'class_name': 'adam',\n",
        "        'config': {\n",
        "            'learning_rate': params.learning_rate\n",
        "        }\n",
        "    }\n",
        "    optimizer = tf.keras.optimizers.get(optimizer_config)\n",
        "\n",
        "\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['sparse_categorical_accuracy'])\n",
        "    return model\n",
        "\n",
        "def _train(params, strategy, writer):\n",
        "    num_workers = strategy.num_replicas_in_sync or 1\n",
        "\n",
        "    TRAIN_BATCH_SIZE = params.batch_size * num_workers\n",
        "    VAL_BATCH_SIZE = params.batch_size * num_workers\n",
        "    logging.info(f\"Running with {TRAIN_BATCH_SIZE} train batch size and {VAL_BATCH_SIZE} validation batch size.\")\n",
        "\n",
        "    (train_data, val_data), mnist_info = tfds.load(\"mnist\",\n",
        "                                                   try_gcs=True,\n",
        "                                                   with_info=True,\n",
        "                                                   split=['train', 'test'],\n",
        "                                                   as_supervised=True)\n",
        "\n",
        "    @tf.function\n",
        "    def norm_data(image, label):\n",
        "        return tf.cast(image, tf.float32) / 255., label\n",
        "\n",
        "    TRAIN_STEPS_EPOCH = int(mnist_info.splits['train'].num_examples // TRAIN_BATCH_SIZE)\n",
        "    VAL_STEPS_EPOCH = int(mnist_info.splits['test'].num_examples // VAL_BATCH_SIZE)\n",
        "    logging.info(f\"Running with {TRAIN_STEPS_EPOCH} train steps and {VAL_STEPS_EPOCH} validation steps.\")\n",
        "\n",
        "    ds_options = tf.data.Options()\n",
        "    ds_options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n",
        "\n",
        "    train_ds = (train_data\n",
        "                .with_options(ds_options)\n",
        "                .map(norm_data, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "                .batch(TRAIN_BATCH_SIZE, drop_remainder=True)\n",
        "                .cache()\n",
        "                .repeat(params.num_epochs)\n",
        "                .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "    val_ds = (val_data\n",
        "              .with_options(ds_options)\n",
        "              .map(norm_data, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "              .batch(VAL_BATCH_SIZE, drop_remainder=True)\n",
        "              .cache()\n",
        "              .repeat(params.num_epochs)\n",
        "              .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "    record('dataset_ready', writer)\n",
        "\n",
        "    with strategy.scope():\n",
        "        model = _compile_model(strategy)\n",
        "\n",
        "    model.summary()\n",
        "    record('model_ready', writer)\n",
        "\n",
        "    model.fit(train_ds, validation_data=val_ds,\n",
        "              steps_per_epoch=TRAIN_STEPS_EPOCH, validation_steps=VAL_STEPS_EPOCH,\n",
        "              epochs=params.num_epochs,\n",
        "              callbacks=[\n",
        "                  LossReporterCallback(),\n",
        "                  tf.keras.callbacks.TensorBoard(os.environ['AIP_TENSORBOARD_LOG_DIR'], profile_batch=0)\n",
        "              ])\n",
        "#tf.keras.callbacks.experimental.BackupAndRestore(os.path.join(params.job_dir, 'backups'))\n",
        "    record('model_trained', writer)\n",
        "\n",
        "    model_save_path = _model_save_path(strategy)\n",
        "    logging.info(f'Saving model to {model_save_path}.')\n",
        "    model.save(model_save_path)\n",
        "    record('model_saved', writer)\n",
        "\n",
        "    logging.info('Model training complete.')\n",
        "    record('done', writer)\n",
        "\n",
        "    logging.info(params)\n",
        "    summarize_recorder()\n",
        "\n",
        "\n",
        "def _get_args():\n",
        "    \"\"\"Argument parser.\n",
        "    Returns:\n",
        "    Dictionary of arguments.\n",
        "    \"\"\"\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\n",
        "        '--num-epochs',\n",
        "        type=int,\n",
        "        default=10,\n",
        "        help='number of times to go through the data, default=5')\n",
        "    parser.add_argument(\n",
        "        '--batch-size',\n",
        "        default=100,\n",
        "        type=int,\n",
        "        help='number of records to read during each training step, default=128')\n",
        "    parser.add_argument(\n",
        "        '--learning-rate',\n",
        "        default=.01,\n",
        "        type=float,\n",
        "        help='learning rate for optimizer, default=.01')\n",
        "    parser.add_argument(\n",
        "        '--long-runner',\n",
        "        default='False',\n",
        "        type=str,\n",
        "        help='long running job indicator, default=False')\n",
        "    parser.add_argument(\n",
        "        '--verbosity',\n",
        "        choices=['DEBUG', 'ERROR', 'FATAL', 'INFO', 'WARN'],\n",
        "        default='DEBUG')\n",
        "    return parser.parse_args()\n",
        "\n",
        "def _detect_strategy():\n",
        "    strategy = None\n",
        "    try:\n",
        "        logging.info('TPU_CONFIG:' + str(os.environ.get('TPU_CONFIG')))\n",
        "        logging.info('TF_CONFIG:' + str(os.environ.get('TF_CONFIG')))\n",
        "        tf_config = json.loads(os.environ.get('TF_CONFIG')) if os.environ.get('TF_CONFIG') else None\n",
        "        tpu_config = json.loads(os.environ.get('TPU_CONFIG')) if os.environ.get('TPU_CONFIG') else None\n",
        "        tf_cluster = tf_config['cluster'] if tf_config and 'cluster' in tf_config else {}\n",
        "        worker_count = len(tf_cluster['worker']) if tf_cluster and 'worker' in tf_cluster else 0\n",
        "\n",
        "        if tpu_config:\n",
        "            resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='local')\n",
        "            tf.config.experimental_connect_to_cluster(resolver)\n",
        "            tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "            strategy = tf.distribute.TPUStrategy(resolver)\n",
        "        elif worker_count > 0:\n",
        "            strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
        "        else:\n",
        "            strategy = tf.distribute.MirroredStrategy()\n",
        "    except Exception as e:\n",
        "        logging.error('Could not detect TF and TPU configuration.' + str(e))\n",
        "\n",
        "    return strategy\n",
        "\n",
        "\n",
        "def _fix_os_vars():\n",
        "    if not 'AIP_TENSORBOARD_LOG_DIR' in os.environ:\n",
        "        os.environ['AIP_TENSORBOARD_LOG_DIR'] = os.environ['AIP_MODEL_DIR']\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    params = _get_args()\n",
        "    if params:\n",
        "        tf.get_logger().setLevel(logging.getLevelName(params.verbosity))\n",
        "        logging.basicConfig(level=logging.getLevelName(params.verbosity))\n",
        "\n",
        "    strategy = _detect_strategy()\n",
        "    _fix_os_vars()\n",
        "\n",
        "    if params and strategy:\n",
        "        writer = tf.summary.create_file_writer(os.environ['AIP_TENSORBOARD_LOG_DIR'])\n",
        "        record('program_start', writer)\n",
        "        logging.info(f'Running training program with strategy:{strategy}')\n",
        "        _train(params, strategy, writer)\n",
        "    else:\n",
        "        logging.error('Could not parse parameters and configuration.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyUkHeLH9raO"
      },
      "source": [
        "With training script file we can now launch a training job which uses GPUs and registers the resulting model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VRbfYfFhS4c"
      },
      "outputs": [],
      "source": [
        "vertex_ai_custom_job = aiplatform.CustomTrainingJob(\n",
        "    display_name=JOB_NAME,\n",
        "    script_path='train.py',\n",
        "    container_uri='europe-docker.pkg.dev/vertex-ai/training/tf-tpu.2-8:latest',\n",
        "    requirements=['cloudml-hypertune', 'tensorflow-datasets'],\n",
        "    model_serving_container_image_uri='europe-docker.pkg.dev/vertex-ai/prediction/tf2-gpu.2-11:latest',\n",
        "    model_description='GPU-accelerated MNIST model',\n",
        "    staging_bucket=STAGING_BUCKET\n",
        ")\n",
        "\n",
        "vertex_ai_custom_job.run(\n",
        "    machine_type='cloud-tpu',\n",
        "    replica_count=1,\n",
        "    accelerator_type = 'TPU_V2',\n",
        "    accelerator_count = 8,\n",
        "    args=['--num-epochs=20'],\n",
        "    sync=True\n",
        ")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BAF7kAgzpmZB"
      },
      "source": [
        "## Launch a TPU-accelerated, Container-based Training on Vertex AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVyxwooG-JC6"
      },
      "source": [
        "In this part we build and publish a container image with our training script to Artifact Registry and launch a container-based training job on Vertex AI Training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "BC1pTAd-p35h"
      },
      "outputs": [],
      "source": [
        "%%writefile Dockerfile\n",
        "#@title Write the Dockerfile\n",
        "# Copyright 2023 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "FROM python:3.8-slim\n",
        "\n",
        "RUN apt update && apt install -y wget\n",
        "\n",
        "RUN wget -q https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/tensorflow/tf-2.10.0/tensorflow-2.10.0-cp38-cp38-linux_x86_64.whl\n",
        "RUN pip3 install tensorflow-2.10.0-cp38-cp38-linux_x86_64.whl\n",
        "RUN rm tensorflow-2.10.0-cp38-cp38-linux_x86_64.whl\n",
        "\n",
        "RUN wget -q https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/libtpu/1.4.0/libtpu.so -O /lib/libtpu.so\n",
        "RUN chmod 777 /lib/libtpu.so\n",
        "\n",
        "RUN pip install cloudml-hypertune tensorflow-datasets\n",
        "\n",
        "ENV PYTHONUNBUFFERED=\"true\"\n",
        "\n",
        "COPY train.py /trainer/\n",
        "\n",
        "ENTRYPOINT [\"python3\", \"/trainer/train.py\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Exp6q5kJ-ZxQ"
      },
      "source": [
        "We build the container and publish it to Artifact Registry:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvrpwRxvrnIZ"
      },
      "outputs": [],
      "source": [
        "!gcloud builds submit --tag=eu.gcr.io/$PROJECT_ID/mnist-trainer:$TIMESTAMP-tpu --project=$PROJECT_ID --region=europe-west4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCAydh09-d3y"
      },
      "source": [
        "Now we can use the container to launch a training job and register the resulting model in Vertex AI:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kln_U0JusfF6"
      },
      "outputs": [],
      "source": [
        "vertex_ai_custom_container_job = aiplatform.CustomContainerTrainingJob(\n",
        "    display_name=JOB_NAME,\n",
        "    container_uri=f'eu.gcr.io/{PROJECT_ID}/mnist-trainer:{TIMESTAMP}-tpu',\n",
        "    model_serving_container_image_uri='europe-docker.pkg.dev/vertex-ai/prediction/tf2-gpu.2-11:latest',\n",
        "    model_description='GPU-accelerated MNIST model',\n",
        "    staging_bucket=STAGING_BUCKET\n",
        ")\n",
        "\n",
        "vertex_ai_custom_container_job.run(\n",
        "    machine_type='cloud-tpu',\n",
        "    replica_count=1,\n",
        "    accelerator_type = 'TPU_V2',\n",
        "    accelerator_count = 8,\n",
        "    args=['--num-epochs=20'],\n",
        "    sync=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5CegYBpWtxQi"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
